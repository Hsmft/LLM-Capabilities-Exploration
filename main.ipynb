{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f96861",
   "metadata": {},
   "source": [
    "3.1 Automatic Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225e8209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Using cached ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting anyio (from httpx>=0.27->ollama)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx>=0.27->ollama)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->ollama)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\mehr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.2 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, idna, h11, certifi, annotated-types, pydantic, httpcore, anyio, httpx, ollama\n",
      "\n",
      "   ------ ---------------------------------  2/12 [pydantic-core]\n",
      "   ---------- -----------------------------  3/12 [idna]\n",
      "   ------------- --------------------------  4/12 [h11]\n",
      "   ------------- --------------------------  4/12 [h11]\n",
      "   -------------------- -------------------  6/12 [annotated-types]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   ----------------------- ----------------  7/12 [pydantic]\n",
      "   -------------------------- -------------  8/12 [httpcore]\n",
      "   -------------------------- -------------  8/12 [httpcore]\n",
      "   -------------------------- -------------  8/12 [httpcore]\n",
      "   ------------------------------ ---------  9/12 [anyio]\n",
      "   ------------------------------ ---------  9/12 [anyio]\n",
      "   ------------------------------ ---------  9/12 [anyio]\n",
      "   ------------------------------ ---------  9/12 [anyio]\n",
      "   --------------------------------- ------ 10/12 [httpx]\n",
      "   --------------------------------- ------ 10/12 [httpx]\n",
      "   --------------------------------- ------ 10/12 [httpx]\n",
      "   ------------------------------------ --- 11/12 [ollama]\n",
      "   ---------------------------------------- 12/12 [ollama]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.7.14 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ollama-0.5.1 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Starting tests for model: llama3.2:3b ---\n",
      "============================================================\n",
      "\n",
      "  Loading perplexity model from: G:/amozeshi/Arshad/NLP/HW4/models/llama-3.2-3b-instruct-q8_0.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from G:/amozeshi/Arshad/NLP/HW4/models/llama-3.2-3b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q8_0:  197 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.18 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 282 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3255.90 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB\n",
      "llama_kv_cache_unified: size =  224.00 MiB (  2048 cells,  28 layers,  1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   278.51 MiB\n",
      "llama_context: graph nodes  = 1014\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Llama 3.2 3B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.block_count': '28', 'general.basename': 'Llama-3.2', 'general.finetune': 'Instruct', 'general.size_label': '3B', 'llama.context_length': '131072', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '24', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Perplexity model loaded successfully.\n",
      "\n",
      "  -> Generating story 1/10 in genre: 'science fiction'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  117356.68 ms /   177 tokens (  663.03 ms per token,     1.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =  354653.80 ms /   463 runs   (  765.99 ms per token,     1.31 tokens per second)\n",
      "llama_perf_context_print:       total time =  476339.94 ms /   640 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.2046\n",
      "     Story saved to generated_stories\\story_1_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 2/10 in genre: 'fantasy'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  129734.67 ms /   251 tokens (  516.87 ms per token,     1.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =  220049.23 ms /    43 runs   ( 5117.42 ms per token,     0.20 tokens per second)\n",
      "llama_perf_context_print:       total time =  350943.98 ms /   294 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.2263\n",
      "     Story saved to generated_stories\\story_2_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 3/10 in genre: 'horror'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  149402.69 ms /   227 tokens (  658.16 ms per token,     1.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =  200895.64 ms /    80 runs   ( 2511.20 ms per token,     0.40 tokens per second)\n",
      "llama_perf_context_print:       total time =  352262.21 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.5918\n",
      "     Story saved to generated_stories\\story_3_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 4/10 in genre: 'comedy'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  122611.98 ms /   261 tokens (  469.78 ms per token,     2.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =  329263.71 ms /   113 runs   ( 2913.84 ms per token,     0.34 tokens per second)\n",
      "llama_perf_context_print:       total time =  455490.29 ms /   374 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.2143\n",
      "     Story saved to generated_stories\\story_4_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 5/10 in genre: 'mystery'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  125471.32 ms /   259 tokens (  484.45 ms per token,     2.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  126522.49 ms /   260 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 4.0969\n",
      "     Story saved to generated_stories\\story_5_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 6/10 in genre: 'romance'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  126007.88 ms /   141 tokens (  893.67 ms per token,     1.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =  301024.78 ms /   163 runs   ( 1846.78 ms per token,     0.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  429565.65 ms /   304 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.6244\n",
      "     Story saved to generated_stories\\story_6_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 7/10 in genre: 'thriller'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  129428.17 ms /   223 tokens (  580.40 ms per token,     1.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  335910.20 ms /   346 runs   (  970.84 ms per token,     1.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  470638.50 ms /   569 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.9874\n",
      "     Story saved to generated_stories\\story_7_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 8/10 in genre: 'historical fiction'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  160657.75 ms /   266 tokens (  603.98 ms per token,     1.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  218009.11 ms /   138 runs   ( 1579.78 ms per token,     0.63 tokens per second)\n",
      "llama_perf_context_print:       total time =  381587.77 ms /   404 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.5139\n",
      "     Story saved to generated_stories\\story_8_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 9/10 in genre: 'western'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  142745.20 ms /   244 tokens (  585.02 ms per token,     1.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =  297836.94 ms /   364 runs   (  818.23 ms per token,     1.22 tokens per second)\n",
      "llama_perf_context_print:       total time =  445762.91 ms /   608 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.2983\n",
      "     Story saved to generated_stories\\story_9_llama3.2_3b.txt\n",
      "\n",
      "  -> Generating story 10/10 in genre: 'cyberpunk'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 391 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  117362.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153163.71 ms /   391 tokens (  391.72 ms per token,     2.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =  186282.07 ms /    36 runs   ( 5174.50 ms per token,     0.19 tokens per second)\n",
      "llama_perf_context_print:       total time =  344493.95 ms /   427 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.9597\n",
      "     Story saved to generated_stories\\story_10_llama3.2_3b.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 434 tensors from G:/amozeshi/Arshad/NLP/HW4/models/qwen2.5-3b-instruct-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = other\n",
      "llama_model_loader: - kv   7:                       general.license.name str              = qwen-research\n",
      "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...\n",
      "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3B\n",
      "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-3B\n",
      "llama_model_loader: - kv  13:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  15:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Unloaded perplexity model for 'llama3.2:3b' to free up memory.\n",
      "\n",
      "============================================================\n",
      "--- Starting tests for model: qwen2.5vl:3b ---\n",
      "============================================================\n",
      "\n",
      "  Loading perplexity model from: G:/amozeshi/Arshad/NLP/HW4/models/qwen2.5-3b-instruct-q4_k_m.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.79 GiB (4.99 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  1265.62 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1834.82 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.33.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.34.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.35.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.35.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    72.00 MiB\n",
      "llama_kv_cache_unified: size =   72.00 MiB (  2048 cells,  36 layers,  1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.75 MiB\n",
      "llama_context: graph nodes  = 1410\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen2.5 3B Instruct', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.basename': 'Qwen2.5', 'general.finetune': 'Instruct', 'general.license.name': 'qwen-research', 'general.size_label': '3B', 'general.license': 'other', 'qwen2.attention.head_count_kv': '2', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-3B-Instruct/blob/main/LICENSE', 'general.base_model.count': '1', 'general.base_model.0.name': 'Qwen2.5 3B', 'general.base_model.0.organization': 'Qwen', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-3B', 'qwen2.block_count': '36', 'qwen2.context_length': '32768', 'qwen2.embedding_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Perplexity model loaded successfully.\n",
      "\n",
      "  -> Generating story 1/10 in genre: 'science fiction'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   39111.30 ms /   288 tokens (  135.80 ms per token,     7.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =  331666.35 ms /  1759 runs   (  188.55 ms per token,     5.30 tokens per second)\n",
      "llama_perf_context_print:       total time =  388529.44 ms /  2047 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.4822\n",
      "     Story saved to generated_stories\\story_1_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 2/10 in genre: 'fantasy'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   37936.53 ms /   145 tokens (  261.63 ms per token,     3.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =  360528.85 ms /  1899 runs   (  189.85 ms per token,     5.27 tokens per second)\n",
      "llama_perf_context_print:       total time =  458753.70 ms /  2044 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 1.8394\n",
      "     Story saved to generated_stories\\story_2_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 3/10 in genre: 'horror'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   42479.36 ms /   227 tokens (  187.13 ms per token,     5.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51063.61 ms /   161 runs   (  317.17 ms per token,     3.15 tokens per second)\n",
      "llama_perf_context_print:       total time =  104823.84 ms /   388 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.5822\n",
      "     Story saved to generated_stories\\story_3_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 4/10 in genre: 'comedy'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   22684.00 ms /   323 tokens (   70.23 ms per token,    14.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =  116276.10 ms /   610 runs   (  190.62 ms per token,     5.25 tokens per second)\n",
      "llama_perf_context_print:       total time =  145918.72 ms /   933 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.8012\n",
      "     Story saved to generated_stories\\story_4_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 5/10 in genre: 'mystery'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   39094.56 ms /   328 tokens (  119.19 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =  237913.35 ms /  1136 runs   (  209.43 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  300320.20 ms /  1464 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.0731\n",
      "     Story saved to generated_stories\\story_5_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 6/10 in genre: 'romance'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   43283.42 ms /   261 tokens (  165.84 ms per token,     6.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =  471929.92 ms /  1785 runs   (  264.39 ms per token,     3.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  562548.21 ms /  2046 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.0023\n",
      "     Story saved to generated_stories\\story_6_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 7/10 in genre: 'thriller'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   32564.90 ms /   225 tokens (  144.73 ms per token,     6.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =  311777.83 ms /  1821 runs   (  171.21 ms per token,     5.84 tokens per second)\n",
      "llama_perf_context_print:       total time =  395671.50 ms /  2046 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.2238\n",
      "     Story saved to generated_stories\\story_7_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 8/10 in genre: 'historical fiction'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   38110.02 ms /   228 tokens (  167.15 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =  375350.02 ms /  1816 runs   (  206.69 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =  453047.04 ms /  2044 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.1693\n",
      "     Story saved to generated_stories\\story_8_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 9/10 in genre: 'western'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 360 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   34412.08 ms /   360 tokens (   95.59 ms per token,    10.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =  172842.50 ms /   772 runs   (  223.89 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =  220864.24 ms /  1132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 3.3687\n",
      "     Story saved to generated_stories\\story_9_qwen2.5vl_3b.txt\n",
      "\n",
      "  -> Generating story 10/10 in genre: 'cyberpunk'...\n",
      "  --> Prepended <|endoftext|> token for Qwen model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   39131.21 ms\n",
      "llama_perf_context_print: prompt eval time =   31450.32 ms /   149 tokens (  211.08 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =  134789.42 ms /   741 runs   (  181.90 ms per token,     5.50 tokens per second)\n",
      "llama_perf_context_print:       total time =  186614.54 ms /   890 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Perplexity: 2.6773\n",
      "     Story saved to generated_stories\\story_10_qwen2.5vl_3b.txt\n",
      "\n",
      "  Unloaded perplexity model for 'qwen2.5vl:3b' to free up memory.\n",
      "\n",
      "============================================================\n",
      "FINAL EVALUATION METRICS\n",
      "============================================================\n",
      "\n",
      "Metrics for model: llama3.2:3b\n",
      "-----------------------------------\n",
      "  Data Items Count: 10\n",
      "  Average Inference Time: 66.1869 seconds/item\n",
      "  Reject Rate (RR): 0.00%\n",
      "  Average Perplexity: 2.9718 (lower is better)\n",
      "\n",
      "Metrics for model: qwen2.5vl:3b\n",
      "-----------------------------------\n",
      "  Data Items Count: 10\n",
      "  Average Inference Time: 72.4379 seconds/item\n",
      "  Reject Rate (RR): 0.00%\n",
      "  Average Perplexity: 2.6220 (lower is better)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "import gc\n",
    "\n",
    "# Dictionary for GGUF model paths\n",
    "model_paths = {\n",
    "    \"llama3.2:3b\": \"G:/amozeshi/Arshad/NLP/HW4/models/llama-3.2-3b-instruct-q8_0.gguf\",\n",
    "    \"qwen2.5vl:3b\": \"G:/amozeshi/Arshad/NLP/HW4/models/qwen2.5-3b-instruct-q4_k_m.gguf\"\n",
    "}\n",
    "\n",
    "def calculate_perplexity_with_llama_cpp(text: str, model: Llama) -> float | None:\n",
    "    \"\"\"\n",
    "    Calculates perplexity using the logprobs approach, filtering None values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        outputs = model.create_completion(\n",
    "            prompt=text,\n",
    "            max_tokens=0,\n",
    "            logprobs=True,\n",
    "            echo=True\n",
    "        )\n",
    "        \n",
    "        # Safely get the list of log probabilities\n",
    "        logprobs_data = outputs.get('choices', [{}])[0].get('logprobs')\n",
    "        if not logprobs_data:\n",
    "            return None\n",
    "        \n",
    "        token_logprobs = logprobs_data.get('token_logprobs')\n",
    "        if not token_logprobs:\n",
    "            return None\n",
    "\n",
    "        # --- FIX: Filter out None values to prevent TypeError ---\n",
    "        filtered_logprobs = [lp for lp in token_logprobs if lp is not None]\n",
    "\n",
    "        if not filtered_logprobs:\n",
    "            return None\n",
    "\n",
    "        avg_log_likelihood = np.mean(filtered_logprobs)\n",
    "        perplexity = np.exp(-avg_log_likelihood)\n",
    "        return perplexity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Could not calculate perplexity with llama-cpp: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "random.seed(42)\n",
    "models_to_test = ['llama3.2:3b', 'qwen2.5vl:3b']\n",
    "genres = [\n",
    "    'science fiction', 'fantasy', 'horror', 'comedy', 'mystery',\n",
    "    'romance', 'thriller', 'historical fiction', 'western', 'cyberpunk'\n",
    "]\n",
    "output_dir = 'generated_stories'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# Main Generation Loop \n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Starting tests for model: {model_name} \")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Load the specific GGUF model for perplexity calculation\n",
    "    llm_for_perplexity = None\n",
    "    model_path = model_paths.get(model_name)\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(f\" WARNING: GGUF model path for '{model_name}' not found. Perplexity will NOT be calculated.\")\n",
    "    else:\n",
    "        print(f\"  Loading perplexity model from: {model_path}...\")\n",
    "        llm_for_perplexity = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,\n",
    "            verbose=True,\n",
    "            logits_all=True\n",
    "        )\n",
    "        print(\"  Perplexity model loaded successfully.\")\n",
    "\n",
    "    for i, genre in enumerate(genres):\n",
    "        prompt = f\"\"\"Write a complete short story embodying the main elements of the {genre} genre. The story should feature:\n",
    "        A compelling and attention-grabbing opening that sets the tone.\n",
    "        Well-developed characters whose motivations and personalities suit the genre.\n",
    "        A plot with a clear beginning, middle, and end, enriched with twists suitable for the genre.\n",
    "        Attention to genre-specific themes or settings, utilizing common tropes creatively.\n",
    "        A satisfying conclusion that ties together critical themes and character arcs.\n",
    "        Incorporate imaginative descriptions and dialogue where appropriate, enhancing engagement and depth. The story should be approximately 100 words to allow for detailed scenario building and resolution.\"\"\"\n",
    "        options = {'temperature': round(random.uniform(0.7, 1.2), 2), 'seed': 42}\n",
    "        \n",
    "        print(f\"\\n Generating story {i+1}/{len(genres)} in genre: '{genre}'...\")\n",
    "        try:\n",
    "            response = ollama.generate(model=model_name, prompt=prompt, options=options, stream=False)\n",
    "            generated_story = response['response'].strip()\n",
    "\n",
    "            if not generated_story:\n",
    "                print(\"WARNING: Generated story is empty. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            perplexity_score = None\n",
    "            if llm_for_perplexity:\n",
    "                text_for_perplexity = generated_story\n",
    "                if \"qwen\" in model_name:\n",
    "                    text_for_perplexity = \"<|endoftext|>\" + generated_story\n",
    "                    print(\" Prepended <|endoftext|> token for Qwen model.\")\n",
    "\n",
    "                perplexity_score = calculate_perplexity_with_llama_cpp(text_for_perplexity, llm_for_perplexity)\n",
    "                \n",
    "                if perplexity_score is not None:\n",
    "                    print(f\" Perplexity: {perplexity_score:.4f}\")\n",
    "\n",
    "            # Store results for final report\n",
    "            evaluation_results.append({\n",
    "                \"model\": model_name,\n",
    "                \"genre\": genre,\n",
    "                \"status\": \"success\",\n",
    "                \"inference_time_ns\": response.get('total_duration', 0),\n",
    "                \"perplexity\": perplexity_score\n",
    "            })\n",
    "            \n",
    "            # Save the generated story to a file\n",
    "            safe_model_name = model_name.replace(':', '_').replace('/', '_')\n",
    "            file_name = os.path.join(output_dir, f\"story_{i+1}_{safe_model_name}.txt\")\n",
    "            with open(file_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"MODEL: {model_name}\\n\")\n",
    "                f.write(f\"GENRE: {genre}\\n\")\n",
    "                f.write(f\"TEMPERATURE: {options['temperature']}\\n\")\n",
    "                perplexity_str = f\"{perplexity_score:.4f}\" if perplexity_score is not None else \"N/A\"\n",
    "                f.write(f\"PERPLEXITY: {perplexity_str}\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "                f.write(generated_story)\n",
    "            print(f\"     Story saved to {file_name}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            evaluation_results.append({\n",
    "                \"model\": model_name, \"genre\": genre, \"status\": \"failure\",\n",
    "                \"inference_time_ns\": 0, \"perplexity\": None\n",
    "            })\n",
    "            print(f\"ERROR generating story for {model_name}: {e}\")\n",
    "\n",
    "    # Free up memory\n",
    "    if llm_for_perplexity:\n",
    "        del llm_for_perplexity\n",
    "        gc.collect()\n",
    "        print(f\"\\n  Unloaded perplexity model for '{model_name}' to free up memory.\")\n",
    "\n",
    "# Calculate and Print Final Metrics \n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for model_name in models_to_test:\n",
    "        model_df = results_df[results_df['model'] == model_name].copy()\n",
    "        \n",
    "        if not model_df.empty:\n",
    "            model_df['perplexity'] = pd.to_numeric(model_df['perplexity'], errors='coerce')\n",
    "            successes = model_df[model_df['status'] == 'success'].shape[0]\n",
    "            total_items = len(model_df)\n",
    "            failures = total_items - successes\n",
    "            \n",
    "            avg_time_s = (model_df['inference_time_ns'].sum() / successes) / 1e9 if successes > 0 else 0\n",
    "            reject_rate = (failures / total_items) * 100 if total_items > 0 else 0\n",
    "            avg_perplexity = model_df['perplexity'].mean()\n",
    "\n",
    "            print(f\"\\nMetrics for model: {model_name}\")\n",
    "            print(\"-\" * 35)\n",
    "            print(f\"  Data Items Count: {total_items}\")\n",
    "            print(f\"  Average Inference Time: {avg_time_s:.4f} seconds/item\")\n",
    "            print(f\"  Reject Rate (RR): {reject_rate:.2f}%\")\n",
    "            print(f\"  Average Perplexity: {avg_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e097fc7",
   "metadata": {},
   "source": [
    "3.2 Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 stories to summarize.\n",
      "\n",
      "-> Summarizing 'story_10_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_10_llama3.2_3b.txt\n",
      "-> Summarizing 'story_10_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_10_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_1_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_1_llama3.2_3b.txt\n",
      "-> Summarizing 'story_1_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_1_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_2_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_2_llama3.2_3b.txt\n",
      "-> Summarizing 'story_2_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_2_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_3_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_3_llama3.2_3b.txt\n",
      "-> Summarizing 'story_3_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_3_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_4_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_4_llama3.2_3b.txt\n",
      "-> Summarizing 'story_4_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_4_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_5_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_5_llama3.2_3b.txt\n",
      "-> Summarizing 'story_5_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_5_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_6_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_6_llama3.2_3b.txt\n",
      "-> Summarizing 'story_6_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_6_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_7_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_7_llama3.2_3b.txt\n",
      "-> Summarizing 'story_7_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_7_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_8_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_8_llama3.2_3b.txt\n",
      "-> Summarizing 'story_8_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_8_qwen2.5vl_3b.txt\n",
      "-> Summarizing 'story_9_llama3.2_3b.txt' using model 'llama3.2:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_9_llama3.2_3b.txt\n",
      "-> Summarizing 'story_9_qwen2.5vl_3b.txt' using model 'qwen2.5vl:3b'...\n",
      " Summary saved to generated_summaries\\summary_of_story_9_qwen2.5vl_3b.txt\n",
      "\n",
      "==================================================\n",
      "--- FINAL EVALUATION METRICS (for your report) ---\n",
      "==================================================\n",
      "\n",
      "Metrics for model: llama3.2:3b\n",
      "------------------------------\n",
      "Data Items Count: 10\n",
      "Average Inference Time: 40.7227 seconds/item\n",
      "Reject Rate (RR): 0.00%\n",
      "\n",
      "Metrics for model: qwen2.5vl:3b\n",
      "------------------------------\n",
      "Data Items Count: 10\n",
      "Average Inference Time: 73.1927 seconds/item\n",
      "Reject Rate (RR): 0.00%\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#Configuration\n",
    "stories_dir = 'generated_stories'\n",
    "summaries_dir = 'generated_summaries'\n",
    "\n",
    "if not os.path.exists(summaries_dir):\n",
    "    os.makedirs(summaries_dir)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "#Main Summarization Loop\n",
    "try:\n",
    "    story_files = [f for f in os.listdir(stories_dir) if f.endswith('.txt')]\n",
    "    if not story_files:\n",
    "        print(f\"Error: No story files found in the '{stories_dir}' directory.\")\n",
    "    else:\n",
    "        print(f\"Found {len(story_files)} stories to summarize.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{stories_dir}' does not exist. Please run the story generation script first.\")\n",
    "    story_files = []\n",
    "\n",
    "for filename in story_files:\n",
    "    # Read the original story and its metadata ---\n",
    "    file_path = os.path.join(stories_dir, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        model_name_from_file = lines[0].replace('MODEL: ', '').strip()\n",
    "        story_text = ''.join(lines[5:])\n",
    "    \n",
    "    #Create the summarization prompt ---\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the provided story into a single, concise paragraph that encapsulates the central plot, key characters, and main theme.\n",
    "\n",
    "    To ensure the summary achieves a high ROUGE-1 F1 score, follow these steps:\n",
    "\n",
    "    Identify the main storyline and pivotal scenes that define the beginning, middle, and conclusion.\n",
    "    Pinpoint character development milestones and their impact on the storyline.\n",
    "    Highlight the core message or theme underscored through the narrative.\n",
    "    Where possible, integrate key phrases or important details from the original text to enhance summary quality.\n",
    "    Use clear and straightforward sentences for effective communication of the summary’s ideas, ensuring the paragraph remains comprehensive yet succinct.\n",
    "    IMPORTANT: Your response MUST be ONLY the summary paragraph itself.\n",
    "    Do NOT output any other text, such as \"Here is the summary...\" or any explanation.\n",
    "\n",
    "    The story text is provided below:\n",
    "    “”\" {story_text} “”\"\n",
    "\n",
    "    Your summary should fit under the section labeled “Concise Summary”.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Summarizing '{filename}' using model '{model_name_from_file}' \")\n",
    "\n",
    "    # Generate the summary\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model_name_from_file,\n",
    "            prompt=prompt,\n",
    "            options={'seed': 42, 'temperature': 0.2},\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        #  Store success result\n",
    "        evaluation_results.append({\n",
    "            \"model\": model_name_from_file,\n",
    "            \"task\": \"Summarization\",\n",
    "            \"status\": \"success\",\n",
    "            \"inference_time_ns\": response.get('total_duration', 0)\n",
    "        })\n",
    "\n",
    "        summary_text = response['response']\n",
    "\n",
    "        summary_filename = os.path.join(summaries_dir, f\"summary_of_{filename}\")\n",
    "        with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(summary_text)\n",
    "\n",
    "        print(f\" Summary saved to {summary_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        evaluation_results.append({\n",
    "            \"model\": model_name_from_file,\n",
    "            \"task\": \"Summarization\",\n",
    "            \"status\": \"failure\",\n",
    "            \"inference_time_ns\": 0\n",
    "        })\n",
    "        print(f\"ERROR summarizing {filename}: {e}\")\n",
    "\n",
    "#Calculate and Print Final Metrics\n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    models_in_results = results_df['model'].unique()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- FINAL EVALUATION METRICS (for your report) ---\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for model_name in models_in_results:\n",
    "        model_df = results_df[results_df['model'] == model_name]\n",
    "        if not model_df.empty:\n",
    "            total_items = len(model_df)\n",
    "            failures = len(model_df[model_df['status'] == 'failure'])\n",
    "            successes = total_items - failures\n",
    "            \n",
    "            reject_rate = (failures / total_items) * 100 if total_items > 0 else 0\n",
    "            avg_time_s = (model_df['inference_time_ns'].sum() / successes) / 1e9 if successes > 0 else 0\n",
    "            \n",
    "            print(f\"\\nMetrics for model: {model_name}\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Data Items Count: {total_items}\")\n",
    "            print(f\"Average Inference Time: {avg_time_s:.4f} seconds/item\")\n",
    "            print(f\"Reject Rate (RR): {reject_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a10af",
   "metadata": {},
   "source": [
    "3.3 Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100 items from the NLI dataset.\n",
      "\n",
      "Processing item 1/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      " ERROR with model qwen2.5vl:3b: llama runner process has terminated: exit status 2 (status code: 500)\n",
      "\n",
      "Processing item 2/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 3/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 4/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 5/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 6/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 7/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 8/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 9/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 10/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 11/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 12/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 13/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 14/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 15/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 16/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 17/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 18/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 19/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 20/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 21/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 22/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 23/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 24/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 25/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 26/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 27/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: entails\n",
      "\n",
      "Processing item 28/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 29/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 30/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 31/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 32/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 33/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 34/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 35/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 36/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 37/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 38/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 39/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 40/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 41/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 42/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 43/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 44/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 45/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 46/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 47/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 48/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 49/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 50/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 51/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 52/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 53/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 54/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 55/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 56/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 57/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 58/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 59/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 60/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 61/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 62/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 63/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 64/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 65/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 66/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 67/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 68/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 69/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 70/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 71/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 72/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 73/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 74/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 75/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 76/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 77/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 78/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 79/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 80/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 81/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 82/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 83/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 84/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 85/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 86/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 87/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "Processing item 88/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 89/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 90/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 91/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 92/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 93/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 94/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 95/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: neutral\n",
      "\n",
      "Processing item 96/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 97/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: entails\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: entails\n",
      "\n",
      "Processing item 98/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: neutral\n",
      "  - Model: qwen2.5vl:3b         | Prediction: entails         | Actual: neutral\n",
      "\n",
      "Processing item 99/100...\n",
      "  - Model: llama3.2:3b          | Prediction: entails         | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: neutral         | Actual: contradicts\n",
      "\n",
      "Processing item 100/100...\n",
      "  - Model: llama3.2:3b          | Prediction: contradicts     | Actual: contradicts\n",
      "  - Model: qwen2.5vl:3b         | Prediction: contradicts     | Actual: contradicts\n",
      "\n",
      "==================================================\n",
      "--- FINAL EVALUATION METRICS (for your report) ---\n",
      "==================================================\n",
      "\n",
      "Metrics for model: llama3.2:3b\n",
      "------------------------------\n",
      "Data Items Count: 100\n",
      "Average Inference Time: 16.1285 seconds/item\n",
      "Reject Rate (RR): 0.00%\n",
      "Classification Accuracy: 56.00%\n",
      "\n",
      "Metrics for model: qwen2.5vl:3b\n",
      "------------------------------\n",
      "Data Items Count: 100\n",
      "Average Inference Time: 32.8716 seconds/item\n",
      "Reject Rate (RR): 1.00%\n",
      "Classification Accuracy: 75.76%\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "nli_dataset_path = './nli/nli.csv' \n",
    "models_to_test = ['llama3.2:3b', 'qwen2.5vl:3b']\n",
    "\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Analyze and classify the relationship between the Premise and the Hypothesis provided. Use one of the following labels to signify the relationship:\n",
    "\n",
    "entails - when the Hypothesis logically follows from the Premise.\n",
    "contradicts - when the Hypothesis is logically refuted by the Premise.\n",
    "neutral - when the Hypothesis neither contradicts nor is entailed by the Premise.\n",
    "IMPORTANT: Your answer must include exactly one of these words: entails, contradicts, or neutral. Do NOT include other text or explanations.\n",
    "\n",
    "Please study the examples provided below:\n",
    "Example\n",
    "Premise: “A man is driving a car on a scenic road.”\n",
    "Hypothesis 1: “A man is in a vehicle.”\n",
    "Answer 1: entails\n",
    "\n",
    "Hypothesis 2: “Nobody is driving any vehicle.”\n",
    "Answer 2: contradicts\n",
    "\n",
    "Hypothesis 3: “The road has many sharp turns.”Answer 3: neutral\n",
    "Now classify the following pair:\n",
    "“”\"\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(nli_dataset_path)\n",
    "    print(f\"Successfully loaded {len(df)} items from the NLI dataset.\")\n",
    "    # df = df.head(10) # For faster testing\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at '{nli_dataset_path}'.\")\n",
    "    df = None\n",
    "\n",
    "# Main NLI Processing Loop\n",
    "if df is not None:\n",
    "    for index, row in df.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        gold_label = row['label'] \n",
    "\n",
    "        print(f\"\\nProcessing item {index + 1}/{len(df)}...\")\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            prompt = prompt_template.format(premise=premise, hypothesis=hypothesis)\n",
    "            \n",
    "            try:\n",
    "                response = ollama.generate(\n",
    "                    model=model_name,\n",
    "                    prompt=prompt,\n",
    "                    options={'seed': 42, 'temperature': 0.1},\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                predicted_label = response['response'].strip().lower().split()[0]\n",
    "                evaluation_results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"status\": \"success\",\n",
    "                    \"inference_time_ns\": response.get('total_duration', 0),\n",
    "                    \"is_correct\": 1 if predicted_label == gold_label else 0\n",
    "                })\n",
    "                print(f\"  - Model: {model_name:<20} | Prediction: {predicted_label:<15} | Actual: {gold_label}\")\n",
    "\n",
    "            except Exception as e:\n",
    "            \n",
    "                evaluation_results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"status\": \"failure\",\n",
    "                    \"inference_time_ns\": 0,\n",
    "                    \"is_correct\": 0\n",
    "                })\n",
    "                print(f\" ERROR with model {model_name}: {e}\")\n",
    "\n",
    "# Calculate and Print Final Metrics \n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL EVALUATION METRICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for model_name in models_to_test:\n",
    "        model_df = results_df[results_df['model'] == model_name]\n",
    "        \n",
    "        if not model_df.empty:\n",
    "            total_items = len(model_df)\n",
    "            failures = len(model_df[model_df['status'] == 'failure'])\n",
    "            successes = total_items - failures\n",
    "\n",
    "            reject_rate = (failures / total_items) * 100 if total_items > 0 else 0\n",
    "            avg_time_s = (model_df['inference_time_ns'].sum() / successes) / 1e9 if successes > 0 else 0\n",
    "            accuracy = (model_df['is_correct'].sum() / successes) * 100 if successes > 0 else 0\n",
    "\n",
    "            print(f\"\\nMetrics for model: {model_name}\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Data Items Count: {total_items}\")\n",
    "            print(f\"Average Inference Time: {avg_time_s:.4f} seconds/item\")\n",
    "            print(f\"Reject Rate (RR): {reject_rate:.2f}%\")\n",
    "            print(f\"Classification Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34237eec",
   "metadata": {},
   "source": [
    "3.4 Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images to caption.\n",
      "\n",
      "Processing image 1/100: ic-001.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 2/100: ic-002.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 3/100: ic-003.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 4/100: ic-004.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 5/100: ic-005.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 6/100: ic-006.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 7/100: ic-007.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 8/100: ic-008.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 9/100: ic-009.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 10/100: ic-010.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 11/100: ic-011.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 12/100: ic-012.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 13/100: ic-013.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 14/100: ic-014.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 15/100: ic-015.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 16/100: ic-016.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 17/100: ic-017.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 18/100: ic-018.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 19/100: ic-019.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 20/100: ic-020.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 21/100: ic-021.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 22/100: ic-022.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 23/100: ic-023.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 24/100: ic-024.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 25/100: ic-025.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 26/100: ic-026.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 27/100: ic-027.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 28/100: ic-028.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 29/100: ic-029.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 30/100: ic-030.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 31/100: ic-031.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 32/100: ic-032.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 33/100: ic-033.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 34/100: ic-034.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 35/100: ic-035.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 36/100: ic-036.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 37/100: ic-037.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 38/100: ic-038.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 39/100: ic-039.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 40/100: ic-040.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 41/100: ic-041.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 42/100: ic-042.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 43/100: ic-043.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 44/100: ic-044.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 45/100: ic-045.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 46/100: ic-046.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 47/100: ic-047.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 48/100: ic-048.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 49/100: ic-049.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 50/100: ic-050.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 51/100: ic-051.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 52/100: ic-052.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 53/100: ic-053.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 54/100: ic-054.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 55/100: ic-055.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 56/100: ic-056.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 57/100: ic-057.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 58/100: ic-058.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 59/100: ic-059.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 60/100: ic-060.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 61/100: ic-061.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 62/100: ic-062.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 63/100: ic-063.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 64/100: ic-064.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 65/100: ic-065.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 66/100: ic-066.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 67/100: ic-067.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 68/100: ic-068.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 69/100: ic-069.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 70/100: ic-070.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 71/100: ic-071.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 72/100: ic-072.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 73/100: ic-073.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 74/100: ic-074.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 75/100: ic-075.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 76/100: ic-076.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 77/100: ic-077.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 78/100: ic-078.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 79/100: ic-079.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 80/100: ic-080.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 81/100: ic-081.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 82/100: ic-082.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 83/100: ic-083.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 84/100: ic-084.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 85/100: ic-085.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 86/100: ic-086.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 87/100: ic-087.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 88/100: ic-088.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 89/100: ic-089.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 90/100: ic-090.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 91/100: ic-091.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 92/100: ic-092.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 93/100: ic-093.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 94/100: ic-094.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 95/100: ic-095.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 96/100: ic-096.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 97/100: ic-097.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 98/100: ic-098.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 99/100: ic-099.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "Processing image 100/100: ic-100.jpg\n",
      " Caption generated and saved.\n",
      "\n",
      "==================================================\n",
      "--- EVALUATION FOR IMAGE CAPTIONING ---\n",
      "==================================================\n",
      "\n",
      "Metrics for model: qwen2.5vl:3b\n",
      "------------------------------\n",
      "Data Items Count: 100\n",
      "Average Inference Time: 67.4034 seconds/item\n",
      "Reject Rate (RR): 0.00%\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "vlm_model_name = 'qwen2.5vl:3b'  \n",
    "\n",
    "image_dataset_path = './ic/images'\n",
    "\n",
    "output_dir = 'generated_captions'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# Get the list of image files\n",
    "try:\n",
    "    image_files = [f for f in os.listdir(image_dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not image_files:\n",
    "        print(f\"Error: No image files found in '{image_dataset_path}'. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} images to caption.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Image directory not found at '{image_dataset_path}'.\")\n",
    "    image_files = []\n",
    "\n",
    "#Main Image Captioning Loop \n",
    "for i, image_filename in enumerate(image_files):\n",
    "    image_path = os.path.join(image_dataset_path, image_filename)\n",
    "    print(f\"\\nProcessing image {i + 1}/{len(image_files)}: {image_filename}\")\n",
    "    \n",
    "    prompt = \"\"\"Generate a high-quality, single-sentence caption for this image. The caption should be factual and clearly describe the main subject, action, and setting.\"\"\"\n",
    "    try:\n",
    "    \n",
    "        response = ollama.generate(\n",
    "            model=vlm_model_name,\n",
    "            prompt=prompt,\n",
    "            images=[image_path],  \n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Record success for evaluation\n",
    "        evaluation_results.append({\n",
    "            \"model\": vlm_model_name, \"task\": \"Image Captioning\", \"status\": \"success\",\n",
    "            \"inference_time_ns\": response.get('total_duration', 0)\n",
    "        })\n",
    "        \n",
    "        # Save the generated caption\n",
    "        caption_text = response['response']\n",
    "        caption_filename = os.path.join(output_dir, f\"{os.path.splitext(image_filename)[0]}_caption.txt\")\n",
    "        with open(caption_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(caption_text)\n",
    "        \n",
    "        print(f\" Caption generated and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        #Record failure for evaluation\n",
    "        evaluation_results.append({\n",
    "            \"model\": vlm_model_name, \"task\": \"Image Captioning\", \"status\": \"failure\",\n",
    "            \"inference_time_ns\": 0\n",
    "        })\n",
    "        print(f\"ERROR processing {image_filename}: {e}\")\n",
    "\n",
    "# Calculate and Print Final Metrics \n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n EVALUATION FOR IMAGE CAPTIONING\\n\" + \"=\"*50)\n",
    "    \n",
    "    model_df = results_df[results_df['model'] == vlm_model_name]\n",
    "    if not model_df.empty:\n",
    "        total_items = len(model_df)\n",
    "        failures = len(model_df[model_df['status'] == 'failure'])\n",
    "        successes = total_items - failures\n",
    "        \n",
    "        reject_rate = (failures / total_items) * 100 if total_items > 0 else 0\n",
    "        avg_time_s = (model_df['inference_time_ns'].sum() / successes) / 1e9 if successes > 0 else 0\n",
    "        \n",
    "        print(f\"\\nMetrics for model: {vlm_model_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Data Items Count: {total_items}\")\n",
    "        print(f\"Average Inference Time: {avg_time_s:.4f} seconds/item\")\n",
    "        print(f\"Reject Rate (RR): {reject_rate:.2f}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aad5b1",
   "metadata": {},
   "source": [
    "3.5 Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b9dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100 items from the VQA dataset.\n",
      "\n",
      "Processing item 1/100: vqa-001.jpg\n",
      "  - Model Prediction: A | Correct Answer: C\n",
      "\n",
      "Processing item 2/100: vqa-002.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 3/100: vqa-003.jpg\n",
      "  - Model Prediction: A | Correct Answer: E\n",
      "\n",
      "Processing item 4/100: vqa-004.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 5/100: vqa-005.jpg\n",
      "  - Model Prediction: A | Correct Answer: B\n",
      "\n",
      "Processing item 6/100: vqa-006.jpg\n",
      "  - Model Prediction: A | Correct Answer: B\n",
      "\n",
      "Processing item 7/100: vqa-007.jpg\n",
      "  - Model Prediction: E | Correct Answer: C\n",
      "\n",
      "Processing item 8/100: vqa-008.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 9/100: vqa-009.jpg\n",
      "  - Model Prediction: E | Correct Answer: D\n",
      "\n",
      "Processing item 10/100: vqa-010.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 11/100: vqa-011.jpg\n",
      "  - Model Prediction: A | Correct Answer: A\n",
      "\n",
      "Processing item 12/100: vqa-012.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 13/100: vqa-013.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 14/100: vqa-014.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 15/100: vqa-015.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 16/100: vqa-016.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 17/100: vqa-017.jpg\n",
      "  - Model Prediction: E | Correct Answer: B\n",
      "\n",
      "Processing item 18/100: vqa-018.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 19/100: vqa-019.jpg\n",
      "  - Model Prediction: A | Correct Answer: E\n",
      "\n",
      "Processing item 20/100: vqa-020.jpg\n",
      "  - Model Prediction: D | Correct Answer: C\n",
      "\n",
      "Processing item 21/100: vqa-021.jpg\n",
      "  - Model Prediction: D | Correct Answer: C\n",
      "\n",
      "Processing item 22/100: vqa-022.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 23/100: vqa-023.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 24/100: vqa-024.jpg\n",
      "  - Model Prediction: E | Correct Answer: C\n",
      "\n",
      "Processing item 25/100: vqa-025.jpg\n",
      "  - Model Prediction: E | Correct Answer: E\n",
      "\n",
      "Processing item 26/100: vqa-026.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 27/100: vqa-027.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 28/100: vqa-028.jpg\n",
      "  - Model Prediction: A | Correct Answer: A\n",
      "\n",
      "Processing item 29/100: vqa-029.jpg\n",
      "  - Model Prediction: E | Correct Answer: B\n",
      "\n",
      "Processing item 30/100: vqa-030.jpg\n",
      "  - Model Prediction: D | Correct Answer: E\n",
      "\n",
      "Processing item 31/100: vqa-031.jpg\n",
      "  - Model Prediction: A | Correct Answer: B\n",
      "\n",
      "Processing item 32/100: vqa-032.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 33/100: vqa-033.jpg\n",
      "  - Model Prediction: D | Correct Answer: E\n",
      "\n",
      "Processing item 34/100: vqa-034.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 35/100: vqa-035.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 36/100: vqa-036.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 37/100: vqa-037.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 38/100: vqa-038.jpg\n",
      "  - Model Prediction: A | Correct Answer: C\n",
      "\n",
      "Processing item 39/100: vqa-039.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 40/100: vqa-040.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 41/100: vqa-041.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 42/100: vqa-042.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 43/100: vqa-043.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 44/100: vqa-044.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 45/100: vqa-045.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 46/100: vqa-046.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 47/100: vqa-047.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 48/100: vqa-048.jpg\n",
      "  - Model Prediction: A | Correct Answer: A\n",
      "\n",
      "Processing item 49/100: vqa-049.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 50/100: vqa-050.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 51/100: vqa-051.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 52/100: vqa-052.jpg\n",
      "  - Model Prediction: D | Correct Answer: C\n",
      "\n",
      "Processing item 53/100: vqa-053.jpg\n",
      "  - Model Prediction: E | Correct Answer: B\n",
      "\n",
      "Processing item 54/100: vqa-054.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 55/100: vqa-055.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 56/100: vqa-056.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 57/100: vqa-057.jpg\n",
      "  - Model Prediction: C | Correct Answer: E\n",
      "\n",
      "Processing item 58/100: vqa-058.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 59/100: vqa-059.jpg\n",
      "  - Model Prediction: C | Correct Answer: D\n",
      "\n",
      "Processing item 60/100: vqa-060.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 61/100: vqa-061.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 62/100: vqa-062.jpg\n",
      "  - Model Prediction: A | Correct Answer: D\n",
      "\n",
      "Processing item 63/100: vqa-063.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 64/100: vqa-064.jpg\n",
      "  - Model Prediction: C | Correct Answer: B\n",
      "\n",
      "Processing item 65/100: vqa-065.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 66/100: vqa-066.jpg\n",
      "  - Model Prediction: B | Correct Answer: A\n",
      "\n",
      "Processing item 67/100: vqa-067.jpg\n",
      "  - Model Prediction: A | Correct Answer: E\n",
      "\n",
      "Processing item 68/100: vqa-068.jpg\n",
      "  - Model Prediction: E | Correct Answer: D\n",
      "\n",
      "Processing item 69/100: vqa-069.jpg\n",
      "  - Model Prediction: B | Correct Answer: D\n",
      "\n",
      "Processing item 70/100: vqa-070.jpg\n",
      "  - Model Prediction: A | Correct Answer: E\n",
      "\n",
      "Processing item 71/100: vqa-071.jpg\n",
      "  - Model Prediction: A | Correct Answer: C\n",
      "\n",
      "Processing item 72/100: vqa-072.jpg\n",
      "  - Model Prediction: B | Correct Answer: E\n",
      "\n",
      "Processing item 73/100: vqa-073.jpg\n",
      "  - Model Prediction: D | Correct Answer: C\n",
      "\n",
      "Processing item 74/100: vqa-074.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 75/100: vqa-075.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 76/100: vqa-076.jpg\n",
      "  - Model Prediction: E | Correct Answer: E\n",
      "\n",
      "Processing item 77/100: vqa-077.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 78/100: vqa-078.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 79/100: vqa-079.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 80/100: vqa-080.jpg\n",
      "  - Model Prediction: A | Correct Answer: C\n",
      "\n",
      "Processing item 81/100: vqa-081.jpg\n",
      "  - Model Prediction: E | Correct Answer: E\n",
      "\n",
      "Processing item 82/100: vqa-082.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 83/100: vqa-083.jpg\n",
      "  - Model Prediction: A | Correct Answer: A\n",
      "\n",
      "Processing item 84/100: vqa-084.jpg\n",
      "  - Model Prediction: E | Correct Answer: A\n",
      "\n",
      "Processing item 85/100: vqa-085.jpg\n",
      "  - Model Prediction: A | Correct Answer: A\n",
      "\n",
      "Processing item 86/100: vqa-086.jpg\n",
      "  - Model Prediction: D | Correct Answer: E\n",
      "\n",
      "Processing item 87/100: vqa-087.jpg\n",
      "  - Model Prediction: A | Correct Answer: B\n",
      "\n",
      "Processing item 88/100: vqa-088.jpg\n",
      "  - Model Prediction: A | Correct Answer: D\n",
      "\n",
      "Processing item 89/100: vqa-089.jpg\n",
      "  - Model Prediction: C | Correct Answer: B\n",
      "\n",
      "Processing item 90/100: vqa-090.jpg\n",
      "  - Model Prediction: B | Correct Answer: A\n",
      "\n",
      "Processing item 91/100: vqa-091.jpg\n",
      "  - Model Prediction: E | Correct Answer: E\n",
      "\n",
      "Processing item 92/100: vqa-092.jpg\n",
      "  - Model Prediction: A | Correct Answer: E\n",
      "\n",
      "Processing item 93/100: vqa-093.jpg\n",
      "  - Model Prediction: D | Correct Answer: A\n",
      "\n",
      "Processing item 94/100: vqa-094.jpg\n",
      "  - Model Prediction: E | Correct Answer: E\n",
      "\n",
      "Processing item 95/100: vqa-095.jpg\n",
      "  - Model Prediction: D | Correct Answer: B\n",
      "\n",
      "Processing item 96/100: vqa-096.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 97/100: vqa-097.jpg\n",
      "  - Model Prediction: D | Correct Answer: D\n",
      "\n",
      "Processing item 98/100: vqa-098.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "Processing item 99/100: vqa-099.jpg\n",
      "  - Model Prediction: B | Correct Answer: C\n",
      "\n",
      "Processing item 100/100: vqa-100.jpg\n",
      "  - Model Prediction: B | Correct Answer: B\n",
      "\n",
      "==================================================\n",
      "--- EVALUATION FOR VISUAL QUESTION ANSWERING ---\n",
      "==================================================\n",
      "\n",
      "Metrics for model: qwen2.5vl:3b\n",
      "------------------------------\n",
      "Data Items Count: 100\n",
      "Average Inference Time: 115.6230 seconds/item\n",
      "Reject Rate (RR): 0.00%\n",
      "Exact Match (EM) Accuracy: 27.00%\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "vlm_model_name = 'qwen2.5vl:3b'  \n",
    "\n",
    "vqa_dataset_path = './vqa/vqa.csv' \n",
    "image_dir = './vqa/images'  \n",
    "output_dir = 'vqa_results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(vqa_dataset_path)\n",
    "    print(f\"Successfully loaded {len(df)} items from the VQA dataset.\")\n",
    "    # df = df.head(5) # For faster testing\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset metadata file not found at '{vqa_dataset_path}'.\")\n",
    "    df = None\n",
    "\n",
    "#Main VQA Loop\n",
    "if df is not None:\n",
    "    for index, row in df.iterrows(): \n",
    "        image_filename = row['image']\n",
    "        \n",
    "        question = row['question']\n",
    "        options_str = row['options']\n",
    "        correct_answer = row['answer'].strip().upper()\n",
    "        \n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        \n",
    "        print(f\"\\nProcessing item {index + 1}/{len(df)}: {image_filename}\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Answer the following multiple-choice question based on the image.\n",
    "        Respond with ONLY the capital letter of the correct option (A, B, C, D, or E). Do not provide any other text or explanation.\n",
    "\n",
    "        Question: {question}\n",
    "        Options: {options_str}\n",
    "\n",
    "        Correct Answer Letter:\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the answer \n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=vlm_model_name,\n",
    "                prompt=prompt,\n",
    "                images=[image_path],\n",
    "                options={'seed': 42, 'temperature': 0.0},\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            #Parse and validate the prediction\n",
    "            prediction = response['response'].strip().upper()\n",
    "            if len(prediction) > 0 and 'A' <= prediction[0] <= 'E':\n",
    "                predicted_letter = prediction[0]\n",
    "                status = \"success\"\n",
    "            else:\n",
    "                predicted_letter = \"invalid_format\"\n",
    "                status = \"failure\" \n",
    "\n",
    "            #Record results \n",
    "            evaluation_results.append({\n",
    "                \"model\": vlm_model_name, \"task\": \"VQA\", \"status\": status,\n",
    "                \"inference_time_ns\": response.get('total_duration', 0),\n",
    "                \"is_correct\": 1 if predicted_letter == correct_answer else 0\n",
    "            })\n",
    "            print(f\"  - Model Prediction: {predicted_letter} | Correct Answer: {correct_answer}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            evaluation_results.append({\n",
    "                \"model\": vlm_model_name, \"task\": \"VQA\", \"status\": \"failure\",\n",
    "                \"inference_time_ns\": 0, \"is_correct\": 0\n",
    "            })\n",
    "            print(f\"ERROR processing {image_filename}: {e}\")\n",
    "\n",
    "#  Calculate and Print Final Metrics \n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n EVALUATION FOR VISUAL QUESTION ANSWERING \\n\" + \"=\"*50)\n",
    "    \n",
    "    total_items = len(results_df)\n",
    "    failures = len(results_df[results_df['status'] == 'failure'])\n",
    "    successes = total_items - failures\n",
    "    \n",
    "    reject_rate = (failures / total_items) * 100 if total_items > 0 else 0\n",
    "    avg_time_s = (results_df['inference_time_ns'].sum() / successes) / 1e9 if successes > 0 else 0\n",
    "    em_accuracy = (results_df['is_correct'].sum() / successes) * 100 if successes > 0 else 0\n",
    "\n",
    "    print(f\"\\nMetrics for model: {vlm_model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Data Items Count: {total_items}\")\n",
    "    print(f\"Average Inference Time: {avg_time_s:.4f} seconds/item\")\n",
    "    print(f\"Reject Rate (RR): {reject_rate:.2f}%\")\n",
    "    print(f\"Exact Match (EM) Accuracy: {em_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdd425",
   "metadata": {},
   "source": [
    "4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42b55b",
   "metadata": {},
   "source": [
    " ROUGE-1 F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdbcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mehr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rouge-score) (2.3.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\mehr\\appdata\\roaming\\python\\python313\\site-packages (from rouge-score) (1.17.0)\n",
      "Collecting click (from nltk->rouge-score)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk->rouge-score)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->rouge-score)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk->rouge-score)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehr\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=4a7928a90bba8a27fef96d12a61120c3051172ed3934b1111e426e20cb34b664\n",
      "  Stored in directory: c:\\users\\mehr\\appdata\\local\\pip\\cache\\wheels\\44\\af\\da\\5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tqdm, regex, joblib, click, absl-py, nltk, rouge-score\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [regex]\n",
      "   ----------- ---------------------------- 2/7 [joblib]\n",
      "   ----------- ---------------------------- 2/7 [joblib]\n",
      "   ----------------- ---------------------- 3/7 [click]\n",
      "   ---------------------- ----------------- 4/7 [absl-py]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------- ----------- 5/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [rouge-score]\n",
      "   ---------------------------------------- 7/7 [rouge-score]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 rouge-score-0.1.2 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce84e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE-1 F1 Scores for Summarization \n",
      "\n",
      "Average ROUGE-1 F1 Score for llama3.2:3b: 0.4667\n",
      "\n",
      "Average ROUGE-1 F1 Score for qwen2.5vl:3b: 0.5630\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "stories_dir = 'generated_stories'\n",
    "summaries_dir = 'generated_summaries'\n",
    "models = ['llama3.2_3b', 'qwen2.5vl_3b'] \n",
    "\n",
    "print(\"Calculating ROUGE-1 F1 Scores for Summarization \")\n",
    "\n",
    "for model_name in models:\n",
    "    scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    \n",
    "    #Find all summaries for the current model\n",
    "    summary_files = [f for f in os.listdir(summaries_dir) if model_name in f]\n",
    "    \n",
    "    for summary_filename in summary_files:\n",
    "        original_story_filename = summary_filename.replace('summary_of_', '')\n",
    "        \n",
    "        story_path = os.path.join(stories_dir, original_story_filename)\n",
    "        summary_path = os.path.join(summaries_dir, summary_filename)\n",
    "        \n",
    "        try:\n",
    "            with open(story_path, 'r', encoding='utf-8') as f:\n",
    "                story_text = ''.join(f.readlines()[5:]) # Skip header\n",
    "\n",
    "            with open(summary_path, 'r', encoding='utf-8') as f:\n",
    "                summary_text = f.read()\n",
    "\n",
    "            # Calculate score\n",
    "            rouge_scores = scorer.score(story_text, summary_text)\n",
    "            scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find matching files for {summary_filename}\")\n",
    "\n",
    "    # Calculate and print the average score for the model\n",
    "    if scores:\n",
    "        average_f1 = sum(scores) / len(scores)\n",
    "        print(f\"\\nAverage ROUGE-1 F1 Score for {model_name.replace('_', ':')}: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d133526",
   "metadata": {},
   "source": [
    "CIDEr (Consensus-based Image Description Eval\n",
    "uation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a0e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.10-cp312-abi3-win_amd64.whl.metadata (1.3 kB)\n",
      "Collecting pycocoevalcap\n",
      "  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mehr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pycocotools) (2.3.2)\n",
      "Using cached pycocotools-2.0.10-cp312-abi3-win_amd64.whl (76 kB)\n",
      "Using cached pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "Installing collected packages: pycocotools, pycocoevalcap\n",
      "\n",
      "   -------------------- ------------------- 1/2 [pycocoevalcap]\n",
      "   -------------------- ------------------- 1/2 [pycocoevalcap]\n",
      "   -------------------- ------------------- 1/2 [pycocoevalcap]\n",
      "   ---------------------------------------- 2/2 [pycocoevalcap]\n",
      "\n",
      "Successfully installed pycocoevalcap-1.2 pycocotools-2.0.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pycocotools pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3928cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ground truth captions from CSV.\n",
      "\n",
      " Calculating CIDEr Score for Image Captioning \n",
      "\n",
      "Overall CIDEr Score: 0.1721\n"
     ]
    }
   ],
   "source": [
    "from pycocoevalcap.cider.cider import Cider\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast \n",
    "\n",
    "captions_dir = 'generated_captions'\n",
    "\n",
    "ground_truth_csv_path = './ic/ic.csv'\n",
    "\n",
    "try:\n",
    "    ground_truth_df = pd.read_csv(ground_truth_csv_path)\n",
    "    print(\"Successfully loaded ground truth captions from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Ground truth file not found at '{ground_truth_csv_path}'\")\n",
    "    ground_truth_df = None\n",
    "\n",
    "if ground_truth_df is not None:\n",
    "    \n",
    "    gts = {}  \n",
    "    res = {}  \n",
    "\n",
    "    for index, row in ground_truth_df.iterrows():\n",
    "        image_filename = row['image']\n",
    "        human_captions_str = row['human_captions']\n",
    "        \n",
    "        image_id = os.path.splitext(image_filename)[0]\n",
    "        gts[image_id] = ast.literal_eval(human_captions_str)\n",
    "        \n",
    "        generated_caption_path = os.path.join(captions_dir, f\"{image_id}_caption.txt\")\n",
    "        try:\n",
    "            with open(generated_caption_path, 'r', encoding='utf-8') as f:\n",
    "                res[image_id] = [f.read().strip()]\n",
    "        except FileNotFoundError:\n",
    "            res[image_id] = [\"\"]\n",
    "\n",
    "    # Calculate CIDEr Score \n",
    "    print(\"\\n Calculating CIDEr Score for Image Captioning \")\n",
    "    cider_scorer = Cider()\n",
    "\n",
    "    (score, scores) = cider_scorer.compute_score(gts, res)\n",
    "\n",
    "    print(f\"\\nOverall CIDEr Score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
